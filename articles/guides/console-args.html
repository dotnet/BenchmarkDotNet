<!DOCTYPE html>
<!--[if IE]><![endif]-->
<html>
  
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <title>How to use console arguments | BenchmarkDotNet </title>
    <meta name="viewport" content="width=device-width">
    <meta name="title" content="How to use console arguments | BenchmarkDotNet ">
    <meta name="generator" content="docfx 2.59.4.0">
    
    <link rel="shortcut icon" href="../../logo/icon-32.png">
    <link rel="stylesheet" href="../../styles/docfx.vendor.css">
    <link rel="stylesheet" href="../../styles/docfx.css">
    <link rel="stylesheet" href="../../styles/main.css">
    <meta property="docfx:navrel" content="../../toc.html">
    <meta property="docfx:tocrel" content="../toc.html">
    
    
    
  
    <!-- Google Analytics -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  
      ga('create', 'UA-41419012-6', 'auto');
      ga('send', 'pageview');
  
    </script>
    <!-- /Google Analytics -->
    <!-- Yandex.Metrika counter -->
    <script type="text/javascript">
        (function (d, w, c) {
            (w[c] = w[c] || []).push(function() {
                try {
                    w.yaCounter40812449 = new Ya.Metrika({
                        id:40812449,
                        clickmap:true,
                        trackLinks:true,
                        accurateTrackBounce:true
                    });
                } catch(e) { }
            });
  
            var n = d.getElementsByTagName("script")[0],
                s = d.createElement("script"),
                f = function () { n.parentNode.insertBefore(s, n); };
            s.type = "text/javascript";
            s.async = true;
            s.src = "https://mc.yandex.ru/metrika/watch.js";
  
            if (w.opera == "[object Opera]") {
                d.addEventListener("DOMContentLoaded", f, false);
            } else { f(); }
        })(document, window, "yandex_metrika_callbacks");
    </script>
    <noscript><div><img src="https://mc.yandex.ru/watch/40812449" style="position:absolute; left:-9999px;" alt=""></div></noscript>
    <!-- /Yandex.Metrika counter -->
    <!-- at the end of the HEAD -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css">
    <link rel="stylesheet" href="../../styles/search.css">
  </head>
  <body data-spy="scroll" data-target="#affix" data-offset="120">
    <div id="wrapper">
      <header>
        
        <nav id="autocollapse" class="navbar navbar-inverse ng-scope" role="navigation">
          <div class="container">
            <div class="navbar-header">
              <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#navbar">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
              </button>
              
              <a class="navbar-brand" href="../../index.html">
                <img id="logo" class="svg" src="../../logo/icon.svg" alt="">
              </a>
            </div>
            <div class="collapse navbar-collapse" id="navbar">
              <div class="navbar-right navbar-form">
                <input type="text" class="form-control" id="search-query" placeholder="Search" autocomplete="off" autofocus="">
              </div>
            </div>
          </div>
        </nav>
        
        <div class="subnav navbar navbar-default">
          <div class="container hide-when-search" id="breadcrumb">
            <ul class="breadcrumb">
              <li></li>
            </ul>
          </div>
        </div>
      </header>
      <div role="main" class="container body-content hide-when-search">
        
        <div class="sidenav hide-when-search">
          <a class="btn toc-toggle collapse" data-toggle="collapse" href="#sidetoggle" aria-expanded="false" aria-controls="sidetoggle">Show / Hide Table of Contents</a>
          <div class="sidetoggle collapse" id="sidetoggle">
            <div id="sidetoc"></div>
          </div>
        </div>
        <div class="article row grid-right">
          <div class="col-md-10">
            <article class="content wrap" id="_content" data-uid="docs.console-args">
<h1 id="how-to-use-console-arguments">How to use console arguments</h1>

<p><code>BenchmarkSwitcher</code> supports various console arguments, to make it work you need to pass the <code>args</code> to switcher:</p>
<pre><code class="lang-cs">class Program
{
    static void Main(string[] args) 
        =&gt; BenchmarkSwitcher.FromAssembly(typeof(Program).Assembly).Run(args);
}
</code></pre>
<p><strong>Note:</strong> the docs that you are currently reading might get outdated, to get the most up-to-date info about supported console arguments run the benchmarks with <code>--help</code>.</p>
<h2 id="filter">Filter</h2>
<p>The <code>--filter</code> or just <code>-f</code> allows you to filter the benchmarks by their full name (<code>namespace.typeName.methodName</code>) using glob patterns.</p>
<p>Examples:</p>
<ol>
<li>Run all benchmarks from System.Memory namespace: <code>-f System.Memory*</code></li>
<li>Run all benchmarks: <code>-f *</code></li>
<li>Run all benchmarks from ClassA and ClassB <code>-f *ClassA* *ClassB*</code></li>
</ol>
<p><strong>Note</strong>: If you would like to <strong>join</strong> all the results into a <strong>single summary</strong>, you need to put <code>--join</code>. For example: <code>-f *ClassA* *ClassB* --join</code></p>
<h2 id="list-of-benchmarks">List of benchmarks</h2>
<p>The <code>--list</code> allows you to print all of the available benchmark names. Available options are:</p>
<ul>
<li><code>flat</code> - prints list of the available benchmarks: <code>--list flat</code></li>
</ul>
<pre><code class="lang-ini">BenchmarkDotNet.Samples.Algo_Md5VsSha256.Md5
BenchmarkDotNet.Samples.Algo_Md5VsSha256.Sha256
BenchmarkDotNet.Samples.IntroArguments.Benchmark
BenchmarkDotNet.Samples.IntroArgumentsSource.SingleArgument
BenchmarkDotNet.Samples.IntroArgumentsSource.ManyArguments
BenchmarkDotNet.Samples.IntroArrayParam.ArrayIndexOf
BenchmarkDotNet.Samples.IntroArrayParam.ManualIndexOf
BenchmarkDotNet.Samples.IntroBasic.Sleep
[...]
</code></pre>
<ul>
<li><code>tree</code> - prints tree of the available benchmarks: <code>--list tree</code></li>
</ul>
<pre><code class="lang-ini">BenchmarkDotNet
 └─Samples
    ├─Algo_Md5VsSha256
    │  ├─Md5
    │  └─Sha256
    ├─IntroArguments
    │  └─Benchmark
    ├─IntroArgumentsSource
    │  ├─SingleArgument
    │  └─ManyArguments
    ├─IntroArrayParam
    │  ├─ArrayIndexOf
    │  └─ManualIndexOf
    ├─IntroBasic
    │  ├─Sleep
[...]
</code></pre>
<p>The <code>--list</code> option works with the <code>--filter</code> option. Examples:</p>
<ul>
<li><code>--list flat --filter *IntroSetupCleanup*</code> prints:</li>
</ul>
<pre><code class="lang-ini">BenchmarkDotNet.Samples.IntroSetupCleanupGlobal.Logic
BenchmarkDotNet.Samples.IntroSetupCleanupIteration.Benchmark
BenchmarkDotNet.Samples.IntroSetupCleanupTarget.BenchmarkA
BenchmarkDotNet.Samples.IntroSetupCleanupTarget.BenchmarkB
BenchmarkDotNet.Samples.IntroSetupCleanupTarget.BenchmarkC
BenchmarkDotNet.Samples.IntroSetupCleanupTarget.BenchmarkD
</code></pre>
<ul>
<li><code>--list tree --filter *IntroSetupCleanup*</code> prints:</li>
</ul>
<pre><code class="lang-ini">BenchmarkDotNet
 └─Samples
    ├─IntroSetupCleanupGlobal
    │  └─Logic
    ├─IntroSetupCleanupIteration
    │  └─Benchmark
    └─IntroSetupCleanupTarget
       ├─BenchmarkA
       ├─BenchmarkB
       ├─BenchmarkC
       └─BenchmarkD
</code></pre>
<h2 id="categories">Categories</h2>
<p>You can also filter the benchmarks by categories:</p>
<ul>
<li><code>--anyCategories</code> - runs all benchmarks that belong to <strong>any</strong> of the provided categories</li>
<li><code>--allCategories</code>- runs all benchmarks that belong to <strong>all</strong> provided categories</li>
</ul>
<h2 id="diagnosers">Diagnosers</h2>
<ul>
<li><code>-m</code>, <code>--memory</code> - enables MemoryDiagnoser and prints memory statistics</li>
<li><code>-t</code>, <code>--threading</code> - enables <code>ThreadingDiagnoser</code> and prints threading statistics</li>
<li><code>-d</code>, <code>--disasm</code>- enables DisassemblyDiagnoser and exports diassembly of benchmarked code. When you enable this option, you can use:
<ul>
<li><code>--disasmDepth</code> - Sets the recursive depth for the disassembler.</li>
<li><code>--disasmDiff</code> - Generates diff reports for the disassembler.</li>
</ul>
</li>
</ul>
<h2 id="runtimes">Runtimes</h2>
<p>The <code>--runtimes</code> or just <code>-r</code> allows you to run the benchmarks for selected Runtimes. Available options are:</p>
<ul>
<li>Clr - BDN will either use Roslyn (if you run it as .NET app) or latest installed .NET SDK to build the benchmarks (if you run it as .NET Core app).</li>
<li>Core - if you run it as .NET Core app, BDN will use the same target framework moniker, if you run it as .NET app it's going to use netcoreapp2.1.</li>
<li>Mono - it's going to use the Mono from <code>$Path</code>, you can override  it with <code>--monoPath</code>.</li>
<li>net46, net461, net462, net47, net471, net472 - to build and run benchmarks against specific .NET framework version.</li>
<li>netcoreapp2.0, netcoreapp2.1, netcoreapp2.2, netcoreapp3.0, netcoreapp3.1, net5.0, net6.0, net7.0 - to build and run benchmarks against specific .NET Core version.</li>
<li>nativeaot5.0, nativeaot6.0, nativeaot7.0 - to build and run benchmarks using NativeAOT. Can be customized with additional options: <code>--ilcPath</code>, <code>--ilCompilerVersion</code>.</li>
</ul>
<p>Example: run the benchmarks for .NET 4.7.2 and .NET Core 2.1:</p>
<pre><code class="lang-log">dotnet run -c Release -- --runtimes net472 netcoreapp2.1
</code></pre>
<p>Example: run the benchmarks for .NET Core 3.0 and latest .NET SDK installed on your PC:</p>
<pre><code class="lang-log">dotnet run -c Release -f netcoreapp3.0 -- --runtimes clr core
</code></pre>
<p>But same command executed with <code>-f netcoreapp2.0</code> is going to run the benchmarks for .NET Core 2.0:</p>
<pre><code class="lang-log">dotnet run -c Release -f netcoreapp2.0 -- --runtimes clr core
</code></pre>
<h2 id="number-of-invocations-and-iterations">Number of invocations and iterations</h2>
<ul>
<li><code>--launchCount</code> - how many times we should launch process with target benchmark. The default is 1.</li>
<li><code>--warmupCount</code> - how many warmup iterations should be performed. If you set it, the minWarmupCount and maxWarmupCount are ignored. By default calculated by the heuristic.</li>
<li><code>--minWarmupCount</code> - minimum count of warmup iterations that should be performed. The default is 6.</li>
<li><code>--maxWarmupCount</code> - maximum count of warmup iterations that should be performed. The default is 50.</li>
<li><code>--iterationTime</code> - desired time of execution of an iteration. Used by Pilot stage to estimate the number of invocations per iteration. 500ms by default.</li>
<li><code>--iterationCount</code> - how many target iterations should be performed. By default calculated by the heuristic.</li>
<li><code>--minIterationCount</code> - minimum number of iterations to run. The default is 15.</li>
<li><code>--maxIterationCount</code> - maximum number of iterations to run. The default is 100.</li>
<li><code>--invocationCount</code> - invocation count in a single iteration. By default calculated by the heuristic.</li>
<li><code>--unrollFactor</code> - how many times the benchmark method will be invoked per one iteration of a generated loop. 16 by default</li>
<li><code>--runOncePerIteration</code> - run the benchmark exactly once per iteration. False by default.</li>
</ul>
<p>Example: run single warmup iteration, from 9 to 12 actual workload iterations.</p>
<pre><code class="lang-log">dotnet run -c Release -- --warmupCount 1 --minIterationCount 9 --maxIterationCount 12
</code></pre>
<h2 id="specifying-custom-default-settings-for-console-argument-parser">Specifying custom default settings for console argument parser</h2>
<p>If you want to have a possibility to specify custom default Job settings programmatically and optionally overwrite it with console line arguments, then you should create a global config with single job marked as <code>.AsDefault</code> and pass it to <code>BenchmarkSwitcher</code> together with the console line arguments.</p>
<p>Example: run single warmup iteration by default.</p>
<pre><code class="lang-cs">static void Main(string[] args)
    =&gt; BenchmarkSwitcher
        .FromAssembly(typeof(Program).Assembly)
        .Run(args, GetGlobalConfig());

static IConfig GetGlobalConfig()
    =&gt; DefaultConfig.Instance
        .With(Job.Default
            .WithWarmupCount(1)
            .AsDefault()); // the KEY to get it working
</code></pre>
<p>Now, the default settings are: <code>WarmupCount=1</code> but you might still overwrite it from console args like in the example below:</p>
<pre><code class="lang-log">dotnet run -c Release -- --warmupCount 2
</code></pre>
<h2 id="statistical-test">Statistical Test</h2>
<p>To perform a Mann–Whitney U Test and display the results in a dedicated column you need to provide the Threshold:</p>
<ul>
<li><code>--statisticalTest</code>- Threshold for Mann–Whitney U Test. Examples: 5%, 10ms, 100ns, 1s</li>
</ul>
<p>Example: run Mann–Whitney U test with relative ratio of 5% for all benchmarks for .NET Core 2.0 (base) vs .NET Core 2.1 (diff). .NET Core 2.0 will be baseline because it was first.</p>
<pre><code class="lang-log">dotnet run -c Release -- --filter * --runtimes netcoreapp2.0 netcoreapp2.1 --statisticalTest 5%
</code></pre>
<h2 id="more">More</h2>
<ul>
<li><code>-j</code>, <code>--job</code> (Default: Default) Dry/Short/Medium/Long or Default.</li>
<li><code>-e</code>, <code>--exporters</code> GitHub/StackOverflow/RPlot/CSV/JSON/HTML/XML.</li>
<li><code>-i</code>, <code>--inProcess</code> (default: false) run benchmarks in the same process, without spawning child process per benchmark.</li>
<li><code>-a</code>, <code>--artifacts</code> valid path to an accessible directory where output artifacts will be stored.</li>
<li><code>--outliers</code> (default: RemoveUpper) <code>DontRemove</code>/<code>RemoveUpper</code>/<code>RemoveLower</code>/<code>RemoveAll</code>.</li>
<li><code>--affinity</code> affinity mask to set for the benchmark process.</li>
<li><code>--allStats</code> (default: false) Displays all statistics (min, max &amp; more).</li>
<li><code>--allCategories</code> categories to run. If few are provided, only the benchmarks which belong to all of them are going to be executed.</li>
<li><code>--attribute</code> run all methods with given attribute (applied to class or method).</li>
<li><code>--monoPath</code> optional path to Mono which should be used for running benchmarks.</li>
<li><code>--cli</code> path to dotnet cli (optional).</li>
<li><code>--packages</code> the directory to restore packages to (optional).</li>
<li><code>--coreRun</code> path(s) to CoreRun (optional).</li>
<li><code>--ilcPath</code> path to ILCompiler for NativeAOT.</li>
<li><code>--info</code> prints environment configuration including BenchmarkDotNet, OS, CPU and .NET version</li>
<li><code>--stopOnFirstError</code> stop on first error.</li>
<li><code>--help</code> display this help screen.</li>
<li><code>--version</code> display version information.</li>
<li><code>--keepFiles</code> (default: false) determines if all auto-generated files should be kept or removed after running the benchmarks.</li>
<li><code>--noOverwrite</code> (default: false) determines if the exported result files should not be overwritten.</li>
<li><code>--disableLogFile</code> disables the log file.</li>
<li><code>--maxWidth</code> max parameter column width, the default is 20.</li>
<li><code>--envVars</code> colon separated environment variables (key:value).</li>
<li><code>--strategy</code> the RunStrategy that should be used. Throughput/ColdStart/Monitoring.</li>
<li><code>--platform</code> the Platform that should be used. If not specified, the host process platform is used (default). AnyCpu/X86/X64/Arm/Arm64/LoongArch64.</li>
<li><code>--runOncePerIteration</code> run the benchmark exactly once per iteration.</li>
<li><code>--buildTimeout</code> build timeout in seconds.</li>
<li><code>--wasmEngine</code> full path to a java script engine used to run the benchmarks, used by Wasm toolchain.</li>
<li><code>--wasmMainJS</code> path to the test-main.js file used by Wasm toolchain. Mandatory when using &quot;--runtimes wasm&quot;</li>
<li><code>--expose_wasm</code> arguments for the JavaScript engine used by Wasm toolchain.</li>
<li><code>--customRuntimePack</code> specify the path to a custom runtime pack. Only used for wasm currently.</li>
</ul>
</article>
          </div>
          
          <div class="hidden-sm col-md-2" role="complementary">
            <div class="sideaffix">
              <div class="contribution">
                <ul class="nav">
                  <li>
                    <a href="https://github.com/dotnet/BenchmarkDotNet/blob/docs-stable/docs/articles/guides/console-args.md/#L1" class="contribution-link">Improve this Doc</a>
                  </li>
                </ul>
              </div>
              <nav class="bs-docs-sidebar hidden-print hidden-xs hidden-sm affix" id="affix">
                <h5>In This Article</h5>
                <div></div>
              </nav>
            </div>
          </div>
        </div>
      </div>
      
      <footer>
        <div class="grad-bottom"></div>
        <div class="footer">
          <div class="container">
            <span class="pull-right">
              <a href="#top">Back to top</a>
            </span>
            Copyright &copy; 2013–2021 .NET Foundation and contributors
            
          </div>
        </div>
      </footer>
    </div>
    
    <script type="text/javascript" src="../../styles/docfx.vendor.js"></script>
    <script type="text/javascript" src="../../styles/docfx.js"></script>
    <script type="text/javascript" src="../../styles/main.js"></script>
    <!-- at the end of the BODY -->
    <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script>
    <script type="text/javascript">
        docsearch({
            apiKey: 'c56d6c558557cfa1fd103a049363cf8b',
            indexName: 'benchmarkdotnet',
            inputSelector: '#search-query',
            debug: false // Set debug to true if you want to inspect the dropdown
        });
    </script>  </body>
</html>
