<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
      <title>How to use console arguments | BenchmarkDotNet </title>
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <meta name="title" content="How to use console arguments | BenchmarkDotNet ">
      
      <link rel="icon" href="../../logo/icon-32.png">
      <link rel="stylesheet" href="../../public/docfx.min.css">
      <link rel="stylesheet" href="../../public/main.css">
      <meta name="docfx:navrel" content="../../toc.html">
      <meta name="docfx:tocrel" content="../toc.html">
      
      <meta name="docfx:rel" content="../../">
      
      
      <meta name="docfx:docurl" content="https://github.com/dotnet/BenchmarkDotNet/blob/docs-stable/docs/articles/guides/console-args.md/#L1">
      <meta name="loc:inThisArticle" content="In this article">
      <meta name="loc:searchResultsCount" content="{count} results for &quot;{query}&quot;">
      <meta name="loc:searchNoResults" content="No results for &quot;{query}&quot;">
      <meta name="loc:tocFilter" content="Filter by title">
      <meta name="loc:nextArticle" content="Next">
      <meta name="loc:prevArticle" content="Previous">
      <meta name="loc:themeLight" content="Light">
      <meta name="loc:themeDark" content="Dark">
      <meta name="loc:themeAuto" content="Auto">
      <meta name="loc:changeTheme" content="Change theme">
      <meta name="loc:copy" content="Copy">
  </head>

  <script type="module">
    import options from './../../public/main.js'
    import { init } from './../../public/docfx.min.js'
    init(options)
  </script>

  <script>
    const theme = localStorage.getItem('theme') || 'auto'
    document.documentElement.setAttribute('data-bs-theme', theme === 'auto' ? (window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light') : theme)
  </script>


  <body class="tex2jax_ignore" data-layout="" data-yaml-mime="">
    <header class="bg-body border-bottom">
      <nav id="autocollapse" class="navbar navbar-expand-md" role="navigation">
        <div class="container-xxl flex-nowrap">
          <a class="navbar-brand" href="../../index.html">
            <img id="logo" class="svg" src="../../logo/icon.svg" alt="">
            
          </a>
          <button class="btn btn-lg d-md-none border-0" type="button" data-bs-toggle="collapse" data-bs-target="#navpanel" aria-controls="navpanel" aria-expanded="false" aria-label="Toggle navigation">
            <i class="bi bi-three-dots"></i>
          </button>
          <div class="collapse navbar-collapse" id="navpanel">
            <div id="navbar">
              <form class="search" role="search" id="search">
                <i class="bi bi-search"></i>
                <input class="form-control" id="search-query" type="search" disabled="" placeholder="Search" autocomplete="off" aria-label="Search">
              </form>
            </div>
          </div>
        </div>
      </nav>
    </header>

    <main class="container-xxl">
      <div class="toc-offcanvas">
        <div class="offcanvas-md offcanvas-start" tabindex="-1" id="tocOffcanvas" aria-labelledby="tocOffcanvasLabel">
          <div class="offcanvas-header">
            <h5 class="offcanvas-title" id="tocOffcanvasLabel">Table of Contents</h5>
            <button type="button" class="btn-close" data-bs-dismiss="offcanvas" data-bs-target="#tocOffcanvas" aria-label="Close"></button>
          </div>
          <div class="offcanvas-body">
            <nav class="toc" id="toc"></nav>
          </div>
        </div>
      </div>

      <div class="content">
        <div class="actionbar">
          <button class="btn btn-lg border-0 d-md-none" style="margin-top: -.65em; margin-left: -.8em" type="button" data-bs-toggle="offcanvas" data-bs-target="#tocOffcanvas" aria-controls="tocOffcanvas" aria-expanded="false" aria-label="Show table of contents">
            <i class="bi bi-list"></i>
          </button>

          <nav id="breadcrumb"></nav>
        </div>

        <article data-uid="docs.console-args">
<h1 id="how-to-use-console-arguments">How to use console arguments</h1>

<p><code>BenchmarkSwitcher</code> supports various console arguments, to make it work you need to pass the <code>args</code> to switcher:</p>
<pre><code class="lang-cs">class Program
{
    static void Main(string[] args) 
        =&gt; BenchmarkSwitcher.FromAssembly(typeof(Program).Assembly).Run(args);
}
</code></pre>
<p><strong>Note:</strong> the docs that you are currently reading might get outdated, to get the most up-to-date info about supported console arguments run the benchmarks with <code>--help</code>.</p>
<h2 id="filter">Filter</h2>
<p>The <code>--filter</code> or just <code>-f</code> allows you to filter the benchmarks by their full name (<code>namespace.typeName.methodName</code>) using glob patterns.</p>
<p>Examples:</p>
<ol>
<li>Run all benchmarks from System.Memory namespace: <code>-f 'System.Memory*'</code></li>
<li>Run all benchmarks: <code>-f '*'</code></li>
<li>Run all benchmarks from ClassA and ClassB <code>-f '*ClassA*' '*ClassB*'</code></li>
</ol>
<p><strong>Note</strong>: If you would like to <strong>join</strong> all the results into a <strong>single summary</strong>, you need to put <code>--join</code>. For example: <code>-f '*ClassA*' '*ClassB*' --join</code></p>
<h2 id="list-of-benchmarks">List of benchmarks</h2>
<p>The <code>--list</code> allows you to print all of the available benchmark names. Available options are:</p>
<ul>
<li><code>flat</code> - prints list of the available benchmarks: <code>--list flat</code></li>
</ul>
<pre><code class="lang-ini">BenchmarkDotNet.Samples.Algo_Md5VsSha256.Md5
BenchmarkDotNet.Samples.Algo_Md5VsSha256.Sha256
BenchmarkDotNet.Samples.IntroArguments.Benchmark
BenchmarkDotNet.Samples.IntroArgumentsSource.SingleArgument
BenchmarkDotNet.Samples.IntroArgumentsSource.ManyArguments
BenchmarkDotNet.Samples.IntroArrayParam.ArrayIndexOf
BenchmarkDotNet.Samples.IntroArrayParam.ManualIndexOf
BenchmarkDotNet.Samples.IntroBasic.Sleep
[...]
</code></pre>
<ul>
<li><code>tree</code> - prints tree of the available benchmarks: <code>--list tree</code></li>
</ul>
<pre><code class="lang-ini">BenchmarkDotNet
 └─Samples
    ├─Algo_Md5VsSha256
    │  ├─Md5
    │  └─Sha256
    ├─IntroArguments
    │  └─Benchmark
    ├─IntroArgumentsSource
    │  ├─SingleArgument
    │  └─ManyArguments
    ├─IntroArrayParam
    │  ├─ArrayIndexOf
    │  └─ManualIndexOf
    ├─IntroBasic
    │  ├─Sleep
[...]
</code></pre>
<p>The <code>--list</code> option works with the <code>--filter</code> option. Examples:</p>
<ul>
<li><code>--list flat --filter *IntroSetupCleanup*</code> prints:</li>
</ul>
<pre><code class="lang-ini">BenchmarkDotNet.Samples.IntroSetupCleanupGlobal.Logic
BenchmarkDotNet.Samples.IntroSetupCleanupIteration.Benchmark
BenchmarkDotNet.Samples.IntroSetupCleanupTarget.BenchmarkA
BenchmarkDotNet.Samples.IntroSetupCleanupTarget.BenchmarkB
BenchmarkDotNet.Samples.IntroSetupCleanupTarget.BenchmarkC
BenchmarkDotNet.Samples.IntroSetupCleanupTarget.BenchmarkD
</code></pre>
<ul>
<li><code>--list tree --filter *IntroSetupCleanup*</code> prints:</li>
</ul>
<pre><code class="lang-ini">BenchmarkDotNet
 └─Samples
    ├─IntroSetupCleanupGlobal
    │  └─Logic
    ├─IntroSetupCleanupIteration
    │  └─Benchmark
    └─IntroSetupCleanupTarget
       ├─BenchmarkA
       ├─BenchmarkB
       ├─BenchmarkC
       └─BenchmarkD
</code></pre>
<h2 id="categories">Categories</h2>
<p>You can also filter the benchmarks by categories:</p>
<ul>
<li><code>--anyCategories</code> - runs all benchmarks that belong to <strong>any</strong> of the provided categories</li>
<li><code>--allCategories</code>- runs all benchmarks that belong to <strong>all</strong> provided categories</li>
</ul>
<h2 id="diagnosers">Diagnosers</h2>
<ul>
<li><code>-m</code>, <code>--memory</code> - enables MemoryDiagnoser and prints memory statistics</li>
<li><code>-t</code>, <code>--threading</code> - enables <code>ThreadingDiagnoser</code> and prints threading statistics</li>
<li><code>-d</code>, <code>--disasm</code>- enables DisassemblyDiagnoser and exports diassembly of benchmarked code. When you enable this option, you can use:
<ul>
<li><code>--disasmDepth</code> - Sets the recursive depth for the disassembler.</li>
<li><code>--disasmDiff</code> - Generates diff reports for the disassembler.</li>
</ul>
</li>
</ul>
<h2 id="runtimes">Runtimes</h2>
<p>The <code>--runtimes</code> or just <code>-r</code> allows you to run the benchmarks for selected Runtimes. Available options are:</p>
<ul>
<li>Clr - BDN will either use Roslyn (if you run it as .NET app) or latest installed .NET SDK to build the benchmarks (if you run it as .NET Core app).</li>
<li>Core - if you run it as .NET Core app, BDN will use the same target framework moniker, if you run it as .NET app it's going to use netcoreapp2.1.</li>
<li>Mono - it's going to use the Mono from <code>$Path</code>, you can override  it with <code>--monoPath</code>.</li>
<li>net46, net461, net462, net47, net471, net472 - to build and run benchmarks against specific .NET framework version.</li>
<li>netcoreapp2.0, netcoreapp2.1, netcoreapp2.2, netcoreapp3.0, netcoreapp3.1, net5.0, net6.0, net7.0 - to build and run benchmarks against specific .NET Core version.</li>
<li>nativeaot5.0, nativeaot6.0, nativeaot7.0 - to build and run benchmarks using NativeAOT. Can be customized with additional options: <code>--ilcPath</code>, <code>--ilCompilerVersion</code>.</li>
</ul>
<p>Example: run the benchmarks for .NET 4.7.2 and .NET Core 2.1:</p>
<pre><code class="lang-log">dotnet run -c Release -- --runtimes net472 netcoreapp2.1
</code></pre>
<p>Example: run the benchmarks for .NET Core 3.0 and latest .NET SDK installed on your PC:</p>
<pre><code class="lang-log">dotnet run -c Release -f netcoreapp3.0 -- --runtimes clr core
</code></pre>
<p>But same command executed with <code>-f netcoreapp2.0</code> is going to run the benchmarks for .NET Core 2.0:</p>
<pre><code class="lang-log">dotnet run -c Release -f netcoreapp2.0 -- --runtimes clr core
</code></pre>
<h2 id="number-of-invocations-and-iterations">Number of invocations and iterations</h2>
<ul>
<li><code>--launchCount</code> - how many times we should launch process with target benchmark. The default is 1.</li>
<li><code>--warmupCount</code> - how many warmup iterations should be performed. If you set it, the minWarmupCount and maxWarmupCount are ignored. By default calculated by the heuristic.</li>
<li><code>--minWarmupCount</code> - minimum count of warmup iterations that should be performed. The default is 6.</li>
<li><code>--maxWarmupCount</code> - maximum count of warmup iterations that should be performed. The default is 50.</li>
<li><code>--iterationTime</code> - desired time of execution of an iteration. Used by Pilot stage to estimate the number of invocations per iteration. 500ms by default.</li>
<li><code>--iterationCount</code> - how many target iterations should be performed. By default calculated by the heuristic.</li>
<li><code>--minIterationCount</code> - minimum number of iterations to run. The default is 15.</li>
<li><code>--maxIterationCount</code> - maximum number of iterations to run. The default is 100.</li>
<li><code>--invocationCount</code> - invocation count in a single iteration. By default calculated by the heuristic.</li>
<li><code>--unrollFactor</code> - how many times the benchmark method will be invoked per one iteration of a generated loop. 16 by default</li>
<li><code>--runOncePerIteration</code> - run the benchmark exactly once per iteration. False by default.</li>
</ul>
<p>Example: run single warmup iteration, from 9 to 12 actual workload iterations.</p>
<pre><code class="lang-log">dotnet run -c Release -- --warmupCount 1 --minIterationCount 9 --maxIterationCount 12
</code></pre>
<h2 id="specifying-custom-default-settings-for-console-argument-parser">Specifying custom default settings for console argument parser</h2>
<p>If you want to have a possibility to specify custom default Job settings programmatically and optionally overwrite it with console line arguments, then you should create a global config with single job marked as <code>.AsDefault</code> and pass it to <code>BenchmarkSwitcher</code> together with the console line arguments.</p>
<p>Example: run single warmup iteration by default.</p>
<pre><code class="lang-cs">static void Main(string[] args)
    =&gt; BenchmarkSwitcher
        .FromAssembly(typeof(Program).Assembly)
        .Run(args, GetGlobalConfig());

static IConfig GetGlobalConfig()
    =&gt; DefaultConfig.Instance
        .With(Job.Default
            .WithWarmupCount(1)
            .AsDefault()); // the KEY to get it working
</code></pre>
<p>Now, the default settings are: <code>WarmupCount=1</code> but you might still overwrite it from console args like in the example below:</p>
<pre><code class="lang-log">dotnet run -c Release -- --warmupCount 2
</code></pre>
<h2 id="response-files-support">Response files support</h2>
<p>Benchmark.NET supports parsing parameters via response files. for example you can create file <code>run.rsp</code> with following content</p>
<pre><code>--warmupCount 1
--minIterationCount 9
--maxIterationCount 12
</code></pre>
<p>and run it using <code>dotnet run -c Release -- @run.rsp</code>. It would be equivalent to running following command line</p>
<pre><code class="lang-log">dotnet run -c Release -- --warmupCount 1 --minIterationCount 9 --maxIterationCount 12
</code></pre>
<h2 id="statistical-test">Statistical Test</h2>
<p>To perform a Mann–Whitney U Test and display the results in a dedicated column you need to provide the Threshold:</p>
<ul>
<li><code>--statisticalTest</code>- Threshold for Mann–Whitney U Test. Examples: 5%, 10ms, 100ns, 1s</li>
</ul>
<p>Example: run Mann–Whitney U test with relative ratio of 5% for all benchmarks for .NET Core 2.0 (base) vs .NET Core 2.1 (diff). .NET Core 2.0 will be baseline because it was first.</p>
<pre><code class="lang-log">dotnet run -c Release -- --filter * --runtimes netcoreapp2.0 netcoreapp2.1 --statisticalTest 5%
</code></pre>
<h2 id="more">More</h2>
<ul>
<li><code>-j</code>, <code>--job</code> (Default: Default) Dry/Short/Medium/Long or Default.</li>
<li><code>-e</code>, <code>--exporters</code> GitHub/StackOverflow/RPlot/CSV/JSON/HTML/XML.</li>
<li><code>-i</code>, <code>--inProcess</code> (default: false) run benchmarks in the same process, without spawning child process per benchmark.</li>
<li><code>-a</code>, <code>--artifacts</code> valid path to an accessible directory where output artifacts will be stored.</li>
<li><code>--outliers</code> (default: RemoveUpper) <code>DontRemove</code>/<code>RemoveUpper</code>/<code>RemoveLower</code>/<code>RemoveAll</code>.</li>
<li><code>--affinity</code> affinity mask to set for the benchmark process.</li>
<li><code>--allStats</code> (default: false) Displays all statistics (min, max &amp; more).</li>
<li><code>--allCategories</code> categories to run. If few are provided, only the benchmarks which belong to all of them are going to be executed.</li>
<li><code>--attribute</code> run all methods with given attribute (applied to class or method).</li>
<li><code>--monoPath</code> optional path to Mono which should be used for running benchmarks.</li>
<li><code>--cli</code> path to dotnet cli (optional).</li>
<li><code>--packages</code> the directory to restore packages to (optional).</li>
<li><code>--coreRun</code> path(s) to CoreRun (optional).</li>
<li><code>--ilcPath</code> path to ILCompiler for NativeAOT.</li>
<li><code>--info</code> prints environment configuration including BenchmarkDotNet, OS, CPU and .NET version</li>
<li><code>--stopOnFirstError</code> stop on first error.</li>
<li><code>--help</code> display this help screen.</li>
<li><code>--version</code> display version information.</li>
<li><code>--keepFiles</code> (default: false) determines if all auto-generated files should be kept or removed after running the benchmarks.</li>
<li><code>--noOverwrite</code> (default: false) determines if the exported result files should not be overwritten.</li>
<li><code>--disableLogFile</code> disables the log file.</li>
<li><code>--maxWidth</code> max parameter column width, the default is 20.</li>
<li><code>--envVars</code> colon separated environment variables (key:value).</li>
<li><code>--strategy</code> the RunStrategy that should be used. Throughput/ColdStart/Monitoring.</li>
<li><code>--platform</code> the Platform that should be used. If not specified, the host process platform is used (default). AnyCpu/X86/X64/Arm/Arm64/LoongArch64.</li>
<li><code>--runOncePerIteration</code> run the benchmark exactly once per iteration.</li>
<li><code>--buildTimeout</code> build timeout in seconds.</li>
<li><code>--wasmEngine</code> full path to a java script engine used to run the benchmarks, used by Wasm toolchain.</li>
<li><code>--wasmMainJS</code> path to the test-main.js file used by Wasm toolchain. Mandatory when using &quot;--runtimes wasm&quot;</li>
<li><code>--expose_wasm</code> arguments for the JavaScript engine used by Wasm toolchain.</li>
<li><code>--customRuntimePack</code> specify the path to a custom runtime pack. Only used for wasm currently.</li>
</ul>
</article>


        <div class="next-article d-print-none border-top" id="nextArticle"></div>
        
      </div>

      <div class="affix">
        <nav id="affix"></nav>
      </div>
    </main>

    <div class="container-xxl search-results" id="search-results"></div>

    <footer class="border-top">
      <div class="container-xxl">
        <div class="flex-fill">
          Copyright &copy; 2013–2023 .NET Foundation and contributors
        </div>
      </div>
    </footer>
  </body>
</html>